B <- 36

# Closed-form formula for the sum of the first n filenames generated by
# filename_for_index.
#
# First, pretend like filename_for_index generates a zero-length
# filename at index 0, and shift the other indices by 1. This doesn't
# affect the sum of lengths, and makes the formula more regular. Now, of
# the first n filenames, all but 1 are at least 1 byte long, all but
# 36+1 are at least 2 bytes long, all but 36²+36+1 are at least 3 bytes
# long, and so on. In general, for a length i, all but 36^(i-1)+...+1 =
# (36^i-1)/35 (a base-36 repunit) filenames are at least i bytes long.
# The greatest value of i that does not exceed the length of the nth
# filename is
#   d = floor(log_36(n/(36/35)))
# For each value of i, we add 1 byte for each filename that is at least
# i bytes long.
#   Σ_i=0…d n - (36^i-1)/35
#   = dn - Σ_i=0…d (36^i-1)/35
#   = dn - ((36^d-1)*36/35² - d/35)
# where the last equality comes from adapting a formula for the sum of
# base-10 repunits: https://oeis.org/A014824.
sum_filename_lengths <- function(n) {
	n <- n + 1 # Shift by 1 index for a fictitious zero-length filename.
	B1 <- B - 1
	# Length of the longest base-B repunit not greater than n
	d <- floor(log(n/(B/B1), B)) + 1
	d*n - ((B^d-1)/B1 * B/B1 - d/B1)
}

triangular_sum_filename_lengths <- Vectorize(function(n) {
	m <- 1
	r <- -1
	s <- 0
	while (B^m <= n) {
		s <- s + m * B^m * (B^m + 1) / 2 + m * r * B^m
		n <- n - B^m
		r <- r + B^m
		m <- m + 1
	}
	s <- s + m * n * (n + 1) / 2 + m * r * n
	s
})

zipped_size <- function(compressed_size, num_additional) {
	size <- 0
	size <- size + num_additional * 5 # 5 is DEFLATE quoting overhead
	size <- size + 16 + compressed_size # 16 is DEFLATE compression overhead
	size <- size + 30 * (1 + num_additional) # Local File Headers
	size <- size + 46 * (1 + num_additional) # Central Directory Headers
	size <- size + 2 * sum_filename_lengths(1 + num_additional) # Filenames in Local File Headers and Central Directory Headers
	size <- size + 22 # EOCD
	size
}

unzipped_size <- function(compressed_size, num_additional) {
	size <- 0
	size <- size + (1 + 1032 + compressed_size * 1032) * (1 + num_additional)
	size <- size + 30 * (num_additional * (num_additional + 1)) / 2
	size <- size + triangular_sum_filename_lengths(1 + num_additional)
	size
}

additional_size <- function(num_additional) {
	num_additional * (30 + 46 + 5) + 2 * sum_filename_lengths(1 + num_additional)
}

optimize <- function(total_size) {
	avail <- total_size - 30 - 46 - 22
	num_additional <- with(list(n=0:(avail/(30+5+46))), {
		which.max(unzipped_size(avail - additional_size(n), n))
	})
	compressed_size = avail - additional_size(num_additional)
	list(compressed_size=compressed_size, num_additional=num_additional)
}

x <- data.frame(zipped_size=c(), unzipped_size=c())
for (n in seq(200, 90000, 1000)) {
	params <- optimize(n)
	x <- rbind(x, data.frame(zipped_size=n, unzipped_size=unzipped_size(params$compressed_size, params$num_additional)))
}
plot(x$zipped_size, x$unzipped_size)

params <- optimize(42374)
params
zipped_size(params$compressed_size, params$num_additional)

# 2^32 - 1 is the maximum representable file size.
# 30*65534 is the file size increase from quoting 65534 Local File Headers.
# sum_filename_lengths(65534) - sum_filename_lengths(1) is the file size increase from quoting all but the first filename.
2^32 - 1 - (30*65534 + sum_filename_lengths(65535) - sum_filename_lengths(1))
