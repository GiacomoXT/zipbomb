B <- 36

# Closed-form formula for sum(nchar(filename_for_index(0..(n-1)))), the sum of
# the first n filenames generated by filename_for_index.
#
# First, pretend like filename_for_index generates a zero-length
# filename at index 0, and shift the other indices by 1. This doesn't
# affect the sum of lengths, and makes the formula more regular. Now, of
# the first n filenames, all but 1 are at least 1 byte long, all but
# 36+1 are at least 2 bytes long, all but 36²+36+1 are at least 3 bytes
# long, and so on. In general, for a length i, all but 36^(i-1)+...+1 =
# (36^i-1)/35 (a base-36 repunit) filenames are at least i bytes long.
# The greatest value of i that does not exceed the length of the nth
# filename is
#   d = floor(log_36(n/(36/35)))
# For each value of i, we add 1 byte for each filename that is at least
# i bytes long.
#   Σ_i=0…d n - (36^i-1)/35
#   = dn - Σ_i=0…d (36^i-1)/35
#   = dn - ((36^d-1)*36/35² - d/35)
# where the last equality comes from adapting a formula for the sum of
# base-10 repunits: https://oeis.org/A014824.
sum_filename_lengths <- function(n) {
	n <- n + 1 # Shift by 1 index for a fictitious zero-length filename.
	B1 <- B - 1
	# Length of the longest base-B repunit not greater than n
	d <- floor(log(n/(B/B1), B)) + 1
	d*n - ((B^d-1)/B1 * B/B1 - d/B1)
}

# Calculates sum(nchar(filename_for_index(i)) * i) for i in 0..(n-1), but in a
# faster way (O(log n)) than just doing the sum.
triangular_sum_filename_lengths <- Vectorize(function(n) {
	s <- 0
	m <- 1
	r <- -1
	# s is the running sum. m is the current filename length (starts at 1).
	# r keeps track of the starting point for weighting the summation in
	# each iteration; it starts at -1, not 0, because we are weighting the
	# filenames by 0:(n-1), not 1:n.
	while (n > 0) {
		# x is the number of filenames of length m: either B^m, or
		# however many remain.
		x <- min(n, B^m)
		# x * (x+1)/2 is 1 + 2 + ... + x.
		# x * ((x+1)/2 + r) is (r+1) + (r+2) + ... + (r+x).
		s <- s + m * x * ((x + 1) / 2 + r)
		# We've handled all x filenames of length m.
		n <- n - x
		# Next iteration, offset weights by the number of filenames we
		# have just processed.
		r <- r + x
		# Next filename length.
		m <- m + 1
	}
	s
})

zipped_size_given_compressed_size <- function(compressed_size, num_additional) {
	zipped_size <- 0
	zipped_size <- zipped_size + num_additional * 5 # 5 is DEFLATE quoting overhead
	zipped_size <- zipped_size + compressed_size
	zipped_size <- zipped_size + 30 * (1 + num_additional) # Local File Headers
	zipped_size <- zipped_size + 46 * (1 + num_additional) # Central Directory Headers
	zipped_size <- zipped_size + 2 * sum_filename_lengths(1 + num_additional) # Filenames in Local File Headers and Central Directory Headers
	zipped_size <- zipped_size + 22 # EOCD
	zipped_size
}

unzipped_size_given_compressed_size <- function(compressed_size, num_additional) {
	unzipped_size <- 0
	unzipped_size <- unzipped_size + uncompressed_size_given_compressed_size(compressed_size) * (1 + num_additional)
	unzipped_size <- unzipped_size + 30 * (num_additional * (num_additional + 1)) / 2
	unzipped_size <- unzipped_size + triangular_sum_filename_lengths(1 + num_additional)
	unzipped_size
}

zipped_size_given_max_uncompressed_size <- function(max_uncompressed_size, num_additional) {
	uncompressed_size <- uncompressed_size_given_max_uncompressed_size(max_uncompressed_size)
	compressed_size <- 0
	# Account for prefix.
	uncompressed_size <- uncompressed_size - 1 - 258 - 258
	compressed_size <- compressed_size + 15
	# We have room for 2 258 blocks in the suffix without requiring an extra byte.
	if (uncompressed_size %% 1032 <= 258*2) {
		compressed_size <- compressed_size + 1 # suffix is 1 byte
	} else {
		compressed_size <- compressed_size + 2 # suffix is 2 byte
	}
	compressed_size <- compressed_size + uncompressed_size %/% 1032
	zipped_size_given_compressed_size(compressed_size, num_additional)
}

unzipped_size_given_max_uncompressed_size <- function(max_uncompressed_size, num_additional) {
	uncompressed_size <- uncompressed_size_given_max_uncompressed_size(max_uncompressed_size)
	unzipped_size <- 0
	unzipped_size <- unzipped_size + uncompressed_size * (1 + num_additional)
	unzipped_size <- unzipped_size + 30 * (num_additional * (num_additional + 1)) / 2
	unzipped_size <- unzipped_size + triangular_sum_filename_lengths(1 + num_additional)
	unzipped_size
}

zipped_size_given_compressed_size_zip64 <- function(compressed_size, num_additional) {
	zipped_size <- 0
	zipped_size <- zipped_size + num_additional * 5 # 5 is DEFLATE quoting overhead
	zipped_size <- zipped_size + compressed_size
	zipped_size <- zipped_size + (30+20) * (1 + num_additional) # Local File Headers
	zipped_size <- zipped_size + (46+12) * (1 + num_additional) # Central Directory Headers
	zipped_size <- zipped_size + 2 * sum_filename_lengths(1 + num_additional) # Filenames in Local File Headers and Central Directory Headers
	zipped_size <- zipped_size + 56 # Zip64 EOCD
	zipped_size <- zipped_size + 20 # Zip64 end of central directory locator
	zipped_size <- zipped_size + 22 # EOCD
	# Assumes that every file gets Zip64 extra info; i.e., that
	# uncompressed_size is at least 0x100000000. And that there is a Zip64
	# EOCD; i.e., that there are at least 0x10000 files.
	ifelse(uncompressed_size_given_compressed_size(compressed_size) > 0xffffffff & (1 + num_additional) > 0xffff, zipped_size, NA)
}

unzipped_size_given_compressed_size_zip64 <- function(compressed_size, num_additional) {
	unzipped_size <- 0
	unzipped_size <- unzipped_size + uncompressed_size_given_compressed_size(compressed_size) * (1 + num_additional)
	unzipped_size <- unzipped_size + (30+20) * (num_additional * (num_additional + 1)) / 2
	unzipped_size <- unzipped_size + triangular_sum_filename_lengths(1 + num_additional)
	ifelse(uncompressed_size_given_compressed_size(compressed_size) > 0xffffffff & (1 + num_additional) > 0xffff, unzipped_size, NA)
}

uncompressed_size_given_compressed_size <- function(compressed_size) {
	# This relies on specific knowledge of how bulk_deflate works,
	# specifically that the prefix and suffix are together 16 bytes long
	# and automatically represent 1033 uncompressed bytes by themselves
	# (1+258+258 in the prefix and 258+258 in the suffix).
	1033 + (compressed_size-16) * 1032
}

# bulk_deflate will get within 258 of max_uncompressed_size (accounting for the
# 1 literal byte at the beginning).
uncompressed_size_given_max_uncompressed_size <- function(max_uncompressed_size) {
	uncompressed_size <- max_uncompressed_size
	uncompressed_size <- uncompressed_size - 1
	uncompressed_size <- uncompressed_size - (uncompressed_size %% 258)
	uncompressed_size <- uncompressed_size + 1
	uncompressed_size
}

additional_size <- function(num_additional) {
	num_additional * (30 + 46 + 5) + 2 * sum_filename_lengths(1 + num_additional)
}

optimize_for_zipped_size <- function(zipped_size) {
	avail <- zipped_size - 30 - 46 - 22
	low <- 0
	high <- avail/(30+5+46)
	num_additional <- (low:high)[[with(list(n=low:high), {
		which.max(unzipped_size_given_compressed_size(avail - additional_size(n), n))
	})]]
	compressed_size <- avail - additional_size(num_additional)
	list(compressed_size=compressed_size, num_additional=num_additional)
}

additional_size_zip64 <- function(num_additional) {
	num_additional * (30 + 20 + 46 + 12 + 5) + 2 * sum_filename_lengths(1 + num_additional)
}

optimize_for_zipped_size_zip64 <- function(zipped_size) {
	avail <- zipped_size - 30 - 20 - 46 - 12 - 56 - 20 - 22
	total <- avail/(30+20+5+46+12)
	low <- floor(total * 0.45)
	high <- floor(total * 0.48)
	num_additional <- (low:high)[[with(list(n=low:high), {
		which.max(unzipped_size_given_compressed_size_zip64(avail - additional_size_zip64(n), n))
	})]]
	if (any(num_additional == low | num_additional == high)) {
		stop()
	}
	compressed_size <- avail - additional_size_zip64(num_additional)
	list(compressed_size=compressed_size, num_additional=num_additional)
}

cat("\n\noptimize zbsm.zip\n");
params <- optimize_for_zipped_size(42374)
params
print(c("zipped size", zipped_size_given_compressed_size(params$compressed_size, params$num_additional)))
print(c("unzipped size", unzipped_size_given_compressed_size(params$compressed_size, params$num_additional)))

cat("\n\noptimize zblg.zip\n");
# 2^32 - 2 is the maximum representable file size. (Not 2^32 - 1 because that makes Go 1.5, at least, insist on a Zip64 extra field being present: https://github.com/golang/go/issues/31692)
# 65533 is the maximum number of additional files. (Not 65534, because yauzl insists on a Zip64 EOCD being present: https://github.com/thejoshwolfe/yauzl/blob/2.10.0/index.js#L140)
# 30*65533 is the file size increase from quoting 65533 Local File Headers.
# sum_filename_lengths(65534) - sum_filename_lengths(1) is the file size increase from quoting all but the first filename.
max_uncompressed_size <- 2^32 - 2 - (30*65533 + sum_filename_lengths(65534) - sum_filename_lengths(1))
# The compression ratio is not monotonic in max_uncompressed_size. Omitting one
# pair of 0 bits decreases the zipped size by 258*65535 ≈ 17 MB, but it is
# worth it if omitting those bits saves one byte in the DEFLATE suffix.
# So try our absolute maximum limit minus 0, 258, 516, 774.
candidates <- seq(max_uncompressed_size, max_uncompressed_size-1032, -258)
max_uncompressed_size <- candidates[[which.max(sapply(candidates, function(x) {
	unzipped_size_given_max_uncompressed_size(x, 65533) / zipped_size_given_max_uncompressed_size(x, 65533)
}))]]
list(max_uncompressed_size=max_uncompressed_size, num_additional=65533)
print(c("zipped size", zipped_size_given_max_uncompressed_size(max_uncompressed_size, 65533)))
print(c("unzipped size", unzipped_size_given_max_uncompressed_size(max_uncompressed_size, 65533)))

cat("\n\noptimize zbxl.zip\n");
# Binary search for zipped_size that gets an unzipped_size as close as possible
# to the full recursive unzipped size of 42.zip.
low <- 10*1024*1024
high <- 100*1024*1024
target <- 4507981343026016
while (low < high) {
	print(c(low, high))
	mid <- floor((low + high) / 2)
	params <- optimize_for_zipped_size_zip64(mid)
	unzipped_size <- unzipped_size_given_compressed_size_zip64(params$compressed_size, params$num_additional)
	if (unzipped_size < target) {
		low <- mid + 1
	} else {
		high <- mid
	}
}
params <- optimize_for_zipped_size_zip64(low)
params
print(c("zipped size", zipped_size_given_compressed_size_zip64(params$compressed_size, params$num_additional)))
print(c("unzipped size", unzipped_size_given_compressed_size_zip64(params$compressed_size, params$num_additional)))

# cat("\n\noptimize zbxxl.zip\n");
# # Binary search for zipped_size that gets an unzipped_size as close as possible
# # to the full recursive unzipped size of 42.zip.
# low <- 10*1024*1024
# high <- NA
# while (is.na(high) || low < high) {
# 	if (is.na(high)) {
# 		mid <- low * 2
# 	} else {
# 		mid <- floor((low + high) / 2)
# 	}
# 	params <- optimize_for_zipped_size_zip64(mid)
# 	unzipped_size <- unzipped_size_given_compressed_size_zip64(params$compressed_size, params$num_additional)
# 	print(c(low, mid, high))
# 	print(log(unzipped_size, 2))
# 	if (log(unzipped_size, 2) < 64) {
# 		low <- mid + 1
# 	} else {
# 		high <- mid
# 	}
# }
# params <- optimize_for_zipped_size_zip64(low)
# params
# print(c("zipped size", zipped_size_given_compressed_size_zip64(params$compressed_size, params$num_additional)))
# print(c("unzipped size", unzipped_size_given_compressed_size_zip64(params$compressed_size, params$num_additional)))
